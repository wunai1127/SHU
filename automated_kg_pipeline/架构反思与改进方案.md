# 知识图谱架构反思与改进方案

## 问题1：跑完我将得到什么？

### 当前版本产出（V1.0）

#### Neo4j图谱结构
```cypher
// 节点示例
(:Entity {
  id: "ent_12345",
  name: "延长缺血时间",
  type: "风险因子",
  properties: {...},
  source_articles: ["PMID123", "PMID456"]
})

// 关系示例
(:Entity {name: "延长缺血时间"})-[:导致 {
  证据强度: "RCT",
  优势比: 2.3,
  P值: 0.001,
  置信区间: "1.5-3.5",
  source_article: "PMID123"
}]->(:Entity {name: "原发性移植物功能障碍"})
```

#### 统计产出
```
output/final/
├── build_report.json          # 构建统计
│   ├── articles_processed: 20547
│   ├── entities_extracted: ~450,000
│   ├── relations_extracted: ~780,000
│   └── entity_type_distribution: {...}
│
├── entities.csv               # 实体备份
├── relations.csv              # 关系备份
└── kg_visualization.html      # 可视化报告
```

#### 质量验证
```
validation/
└── manual_review_samples.json  # 100个人工验证样本
```

---

### ⚠️ 当前版本的问题（用户指出的关键点）

#### 问题A：数值型特征存储不当
**错误做法**（当前可能的输出）：
```cypher
// ❌ 将数值作为独立节点
(:Entity {name: "缺血时间4小时", type: "风险因子"})
(:Entity {name: "主动脉宽度3.2cm", type: "解剖特征"})
```

**正确做法**（应该改进为）：
```cypher
// ✅ 数值作为节点属性（状态变量）
(:Donor {
  id: "donor_001",
  age: 35,
  ischemic_time: 4.0,        // ← GNN训练特征
  aortic_diameter: 3.2,      // ← GNN训练特征
  left_ventricular_hypertrophy: false
})

(:Recipient {
  id: "recip_001",
  age: 55,
  pvr: 4.5,                  // ← GNN训练特征
  lvef: 15,                  // ← GNN训练特征
  creatinine: 1.8
})

// 关系只表达逻辑联系，不包含数值
(:Donor)-[:MATCHED_TO {
  compatibility_score: 0.85,
  ischemic_time: 4.0        // ← 也可以在关系上
}]->(:Recipient)
```

**GNN训练优势**：
```python
# 方便提取特征矩阵
node_features = torch.tensor([
    [age, ischemic_time, aortic_diameter],  # Donor
    [age, pvr, lvef, creatinine]            # Recipient
])

# 错误方式需要复杂查询
```

#### 问题B：图谱不可拓展
当前设计缺少**实体链接**和**关系推理**层。

---

## 问题2：API部分后续修改的隐患 ⚠️ 重点！

### 当前设计的严重问题

#### 问题A：提示词硬编码
```python
# ❌ 当前设计（在代码中）
prompt = f"""请从以下医学摘要中抽取实体和关系...
**实体类型**: {entity_types}
**关系类型**: {relation_types}
"""
```

**后果**：
- 修改提示词需要改代码、重新运行20000篇
- 无法A/B测试不同提示策略
- 无法针对不同文章类型使用不同提示

#### 问题B：只保存解析后的三元组，丢失原始LLM输出
```python
# ❌ 当前设计
cache/triples/{article_id}.json  # 只保存解析后的
```

**后果**：
- LLM输出中可能有额外信息被丢弃
- 如果解析逻辑有bug，无法重新解析
- 无法后续挖掘LLM生成的上下文

#### 问题C：Schema固定，无法动态调整
当前schema固定11种实体类型，但如果发现"灌注液"很重要，需要：
1. 修改schema
2. 重新调用API（20000篇 × 50元）
3. 重新导入

---

### 改进方案：分层架构 + 版本控制

#### 架构V2.0：四层设计

```
┌────────────────────────────────────────────────────────┐
│ Layer 4: 图谱拓展层 (Graph Augmentation)                │
│  - 实体链接 (Entity Linking)                            │
│  - 关系推理 (Relation Inference)                        │
│  - 数值归一化 (Numerical Normalization)                 │
└────────────────────┬───────────────────────────────────┘
                     │
┌────────────────────┴───────────────────────────────────┐
│ Layer 3: 后处理层 (Post-Processing) - 可修改            │
│  - 同义词映射                                           │
│  - 实体去重                                             │
│  - Schema验证                                           │
│  - 数值抽取 → 节点属性                                  │
└────────────────────┬───────────────────────────────────┘
                     │
┌────────────────────┴───────────────────────────────────┐
│ Layer 2: 解析层 (Parsing) - 可修改                      │
│  - JSON解析                                             │
│  - 错误处理                                             │
│  - 格式标准化                                           │
└────────────────────┬───────────────────────────────────┘
                     │
┌────────────────────┴───────────────────────────────────┐
│ Layer 1: LLM调用层 (LLM Call) - 昂贵，尽量不重跑         │
│  - API调用                                              │
│  - 原始输出缓存 ← 关键！                                 │
└────────────────────────────────────────────────────────┘
```

#### 实现：双层缓存

```python
# cache目录结构
cache/
├── llm_raw_outputs/           # ← 新增：原始LLM输出（永久保存）
│   └── {article_id}_v1.json   # 版本化
│       {
│         "prompt_version": "v1.2",
│         "raw_response": "完整的LLM JSON输出",
│         "timestamp": "2025-01-08T10:30:00",
│         "model": "deepseek-chat",
│         "tokens": 823
│       }
│
├── parsed_triples/            # 解析后三元组（可重新生成）
│   └── {article_id}_v2.json   # 可以有多个解析版本
│
└── augmented_graph/           # 拓展后图谱（可重新生成）
    └── {article_id}_v3.json
```

**优势**：
- API调用只做一次（50元）
- 后续所有修改都在本地处理（免费）
- 可以A/B测试不同解析策略
- 可以版本回滚

---

## 问题3：数据库覆盖策略

### 三种模式设计

```yaml
# config.yaml 新增配置
neo4j:
  import_mode: "merge"  # overwrite / append / merge

  merge_strategy:
    entity_deduplication: true     # 实体去重
    relation_aggregation: true     # 关系聚合
    property_merge_policy: "latest"  # latest / merge / keep_existing
```

#### 模式1: 覆盖模式 (overwrite)
```python
# 清空数据库后导入
session.run("MATCH (n:Entity) DETACH DELETE n")
session.run("MATCH (n:Donor) DETACH DELETE n")
# ... 然后导入新数据
```

#### 模式2: 追加模式 (append)
```python
# 直接添加，允许重复
CREATE (n:Entity {...})
```

#### 模式3: 合并模式 (merge) - 推荐
```python
# 智能合并
MERGE (n:Entity {name: "延长缺血时间", type: "风险因子"})
ON CREATE SET n.created = timestamp(), n.source_count = 1
ON MATCH SET n.source_count = n.source_count + 1, n.updated = timestamp()
SET n.sources = n.sources + ["PMID12345"]
```

### 增量更新策略

```python
# 检测已导入的文章
imported_articles = get_imported_article_ids()
new_articles = [a for a in all_articles if a['id'] not in imported_articles]

# 只处理新文章
process_articles(new_articles)
```

---

## 问题4：数值型数据作为节点属性 ✅ 关键！

### 设计原则：数值 = 特征，不是实体

#### Schema V2.0：分离概念节点和实例节点

```json
{
  "node_types": {
    "ConceptNode": {
      "description": "概念性实体（用于推理）",
      "examples": ["风险因子", "并发症", "手术步骤"],
      "properties": {
        "name": "string",
        "type": "string",
        "definition": "string"
      }
    },

    "InstanceNode": {
      "description": "具体实例（用于GNN训练）",
      "examples": ["Donor", "Recipient", "Surgery", "Case"],
      "properties": {
        "numerical_features": "array<float>",  // ← GNN特征向量
        "categorical_features": "array<string>",
        "metadata": "json"
      }
    }
  }
}
```

#### 具体设计：两层图结构

**上层：概念图（用于推理和RAG）**
```cypher
// 概念节点
(:Concept {name: "延长缺血时间", type: "风险因子"})
-[:导致 {证据强度: "RCT", OR: 2.3}]->
(:Concept {name: "原发性移植物功能障碍", type: "并发症"})

// 用于：
// 1. RAG Agent检索："延长缺血时间导致什么？"
// 2. Medical Expert推理："这个风险因子会影响什么？"
```

**下层：实例图（用于GNN训练）**
```cypher
// 实例节点（真实案例）
(:Case {
  id: "case_001",
  outcome: "success",  // 标签
  risk_score: 0.23     // GNN预测目标
})
-[:HAS_DONOR]->
(:Donor {
  age: 35,
  ischemic_time: 4.5,           // ← 特征1
  weight: 70,                   // ← 特征2
  cause_of_death: "trauma",     // ← 特征3（类别）
  donor_type: "DBD"             // ← 特征4
})

(:Case)
-[:HAS_RECIPIENT]->
(:Recipient {
  age: 55,
  pvr: 4.5,                     // ← 特征5
  lvef: 15,                     // ← 特征6
  creatinine: 1.8,              // ← 特征7
  pulmonary_hypertension: true  // ← 特征8
})

(:Case)
-[:HAS_RISK_PROFILE {
  ischemic_time_category: "prolonged",  // 连接到概念层
  pvr_category: "elevated"
}]->
(:RiskProfile)
```

#### GNN训练代码
```python
# 特征矩阵提取（简单！）
def extract_features(case_id):
    query = """
    MATCH (c:Case {id: $case_id})-[:HAS_DONOR]->(d:Donor)
    MATCH (c)-[:HAS_RECIPIENT]->(r:Recipient)
    RETURN
      [d.age, d.ischemic_time, d.weight] AS donor_features,
      [r.age, r.pvr, r.lvef, r.creatinine] AS recipient_features,
      c.outcome AS label
    """
    return session.run(query, case_id=case_id)

# 构建PyG数据
data = Data(
    x=torch.tensor(features),  # [num_nodes, num_features]
    edge_index=edge_index,
    y=torch.tensor(labels)
)
```

### 数值特征标准化

```python
# 后处理：自动检测数值型特征
class NumericalFeatureExtractor:
    """从文本中抽取数值并归一化"""

    def extract_and_normalize(self, text, entity_type):
        # "缺血时间4.5小时" → ischemic_time: 4.5
        # "主动脉宽度3.2cm" → aortic_diameter: 3.2
        # "PVR 4.5 Wood单位" → pvr: 4.5

        patterns = {
            "ischemic_time": r"缺血时间.*?(\d+\.?\d*)\s*小时",
            "pvr": r"PVR.*?(\d+\.?\d*)",
            "lvef": r"LVEF.*?(\d+)%",
            "age": r"(\d+)\s*岁",
            "weight": r"(\d+\.?\d*)\s*kg"
        }

        features = {}
        for feature_name, pattern in patterns.items():
            match = re.search(pattern, text)
            if match:
                features[feature_name] = float(match.group(1))

        return features
```

---

## 问题5：图谱自拓展 - 发现隐藏关联

### 设计：三阶段拓展

#### 阶段1: 实体链接（Entity Linking）

```python
class EntityLinker:
    """发现同一实体的不同表述"""

    def find_duplicate_entities(self):
        # 方法1: 字符串相似度
        duplicates = [
            ("延长缺血时间", "缺血时间延长"),
            ("原发性移植物功能障碍", "PGD"),
            ("肺血管阻力", "PVR")
        ]

        # 方法2: 嵌入相似度
        embeddings = model.encode(entity_names)
        similarity = cosine_similarity(embeddings)

        # 方法3: LLM判断
        prompt = f"以下两个实体是否指同一概念：{e1} vs {e2}"

        return duplicates

    def merge_entities(self, entity1, entity2):
        """合并重复实体"""
        query = """
        MATCH (e1:Entity {name: $name1})
        MATCH (e2:Entity {name: $name2})

        // 合并属性
        SET e1.aliases = e1.aliases + [$name2]
        SET e1.source_count = e1.source_count + e2.source_count

        // 重定向关系
        MATCH (e2)-[r]->(other)
        MERGE (e1)-[new:导致]->(other)
        ON CREATE SET new = properties(r)
        ON MATCH SET new.evidence_count = new.evidence_count + 1

        // 删除e2
        DETACH DELETE e2
        """
```

#### 阶段2: 关系推理（Relation Inference）

```python
class RelationInferencer:
    """推断隐藏的关系"""

    def infer_transitive_relations(self):
        """传递性推理: A→B, B→C => A可能→C"""
        query = """
        MATCH (a)-[:导致]->(b)-[:导致]->(c)
        WHERE NOT (a)-[:导致]->(c)
        RETURN a.name AS cause, c.name AS effect,
               count(*) AS path_count
        HAVING path_count > 3  // 至少3条路径才推断
        """

        # 创建推断关系
        for row in results:
            create_inferred_relation(
                row['cause'],
                row['effect'],
                confidence=row['path_count'] / total_paths,
                type='INFERRED_导致'
            )

    def infer_co_occurrence_relations(self):
        """共现推理：经常一起出现的实体可能有关联"""
        query = """
        MATCH (a)-[:MENTIONED_IN]->(article)<-[:MENTIONED_IN]-(b)
        WHERE a.type = "风险因子" AND b.type = "并发症"
        WITH a, b, count(article) AS co_occurrence
        WHERE co_occurrence > 10
        RETURN a, b, co_occurrence
        """
```

#### 阶段3: 数值聚类（Numerical Clustering）

```python
class NumericalAnalyzer:
    """分析数值型特征，发现隐藏模式"""

    def discover_threshold_rules(self):
        """发现阈值规则"""
        # 分析: 缺血时间在什么值以上风险显著增加？

        query = """
        MATCH (d:Donor)-[:IN_CASE]->(c:Case)
        RETURN d.ischemic_time AS time, c.outcome AS outcome
        """

        # 统计分析
        df = pd.DataFrame(results)

        # 发现阈值
        threshold = find_optimal_threshold(
            df['time'],
            df['outcome']
        )
        # 结果: 4小时

        # 创建概念节点
        create_concept_node(
            name="延长缺血时间",
            definition=f"缺血时间 >= {threshold}小时",
            threshold=threshold
        )

    def discover_risk_patterns(self):
        """聚类发现高风险组合"""
        # 特征: [ischemic_time, pvr, age_diff]
        X = extract_features()

        # 聚类
        kmeans = KMeans(n_clusters=3)
        clusters = kmeans.fit_predict(X)

        # 标注
        for cluster_id in range(3):
            cases_in_cluster = cases[clusters == cluster_id]
            risk_level = calculate_risk(cases_in_cluster)

            create_concept_node(
                name=f"风险组合_{cluster_id}",
                risk_level=risk_level,
                representative_features=kmeans.cluster_centers_[cluster_id]
            )
```

### 自拓展流水线

```python
# automated_kg_pipeline/graph_augmentation.py

class GraphAugmentationPipeline:
    """图谱自拓展流水线"""

    def run(self):
        print("阶段1: 实体链接...")
        duplicates = self.entity_linker.find_duplicates()
        self.merge_duplicates(duplicates)

        print("阶段2: 关系推理...")
        self.relation_inferencer.infer_transitive()
        self.relation_inferencer.infer_co_occurrence()

        print("阶段3: 数值分析...")
        self.numerical_analyzer.discover_thresholds()
        self.numerical_analyzer.discover_patterns()

        print("阶段4: 质量评估...")
        self.evaluate_augmented_graph()
```

---

## 问题6：Schema动态调整 - 灵活发现新实体类型

### 设计：两阶段Schema演化

#### 第一阶段：固定Schema抽取（当前）
```python
# 使用预定义的11种实体类型
entity_types = ["手术步骤", "风险因子", "并发症", ...]
```

#### 第二阶段：动态Schema发现

```python
class SchemEvolutionAnalyzer:
    """Schema演化分析器"""

    def discover_new_entity_types(self):
        """从LLM原始输出中发现新的实体类型"""

        # 读取所有原始LLM输出
        all_entities = []
        for article in articles:
            raw_output = load_raw_llm_output(article['id'])
            entities = parse_entities(raw_output)
            all_entities.extend(entities)

        # 统计未分类的实体
        uncategorized = [
            e for e in all_entities
            if e['type'] not in PREDEFINED_TYPES
        ]

        # 聚类分析
        entity_names = [e['name'] for e in uncategorized]
        embeddings = model.encode(entity_names)

        clusters = DBSCAN(eps=0.3).fit(embeddings)

        # 为每个聚类命名（用LLM）
        new_types = []
        for cluster_id in set(clusters.labels_):
            if cluster_id == -1:  # 噪声
                continue

            cluster_entities = [
                entity_names[i]
                for i, label in enumerate(clusters.labels_)
                if label == cluster_id
            ]

            # 用LLM分析这个聚类
            prompt = f"""
            以下实体属于同一类别，请给这个类别命名：
            {cluster_entities[:20]}
            """

            type_name = llm(prompt)
            new_types.append({
                "type_name": type_name,
                "count": len(cluster_entities),
                "examples": cluster_entities[:10]
            })

        return new_types

    def analyze_entity_importance(self):
        """分析实体重要性（如"灌注液"）"""

        query = """
        MATCH (e:Entity)
        RETURN
          e.type AS type,
          e.name AS name,
          count{(e)-[:导致]->()} AS out_degree,
          count{()-[:导致]->(e)} AS in_degree,
          count{(e)-[:MENTIONED_IN]->()} AS mention_count
        ORDER BY out_degree + in_degree + mention_count DESC
        LIMIT 100
        """

        # 发现高度连接但未分类的实体
        important_uncategorized = [
            row for row in results
            if row['type'] not in CORE_TYPES and
               row['in_degree'] + row['out_degree'] > 10
        ]

        return important_uncategorized
```

### Schema版本管理

```json
// schemas/chinese_medical_kg_schema_v2.json
{
  "version": "2.0",
  "changelog": {
    "2.0": {
      "date": "2025-01-10",
      "changes": [
        "新增实体类型: 灌注液",
        "新增实体类型: 免疫抑制方案",
        "修改: 风险因子 → 风险因子(概念) + 风险特征(数值)"
      ]
    },
    "1.0": {
      "date": "2025-01-08",
      "changes": ["初始版本，11种实体类型"]
    }
  },

  "entity_types": {
    "灌注液": {  // ← 新发现的重要类型
      "description": "心脏保存灌注液",
      "examples": ["HTK液", "UW液", "St. Thomas液"],
      "properties": {
        "成分": "string",
        "温度": "float",
        "渗透压": "float"
      },
      "discovered_by": "schema_evolution_v1",
      "importance_score": 0.87
    }
  }
}
```

### 增量重抽取策略

```python
class IncrementalReExtraction:
    """增量重抽取（只重跑部分文章）"""

    def selective_reextraction(self, new_entity_types):
        """选择性重抽取"""

        # 策略1: 只重抽取提到新实体的文章
        relevant_articles = self.find_articles_mentioning(
            keywords=["灌注液", "HTK", "UW液"]
        )
        # 预计: 500篇 × 0.001元 = 0.5元

        # 策略2: 用零样本提示在原始LLM输出中补充
        for article in relevant_articles:
            raw_output = load_raw_output(article['id'])

            # 补充提示（不重新调用LLM）
            additional_prompt = """
            从上述输出中，额外识别以下实体类型：
            - 灌注液及其属性（成分、温度等）
            """

            # 或者，如果必须重新调用：
            new_output = llm(
                text=article['abstract'],
                schema=updated_schema,  # 包含新类型
                cache_key=f"{article['id']}_v2"  # 新版本
            )
```

---

## 综合改进方案：V2.0架构

### 核心改进点

| 维度 | V1.0（当前） | V2.0（改进） |
|------|------------|------------|
| **数值特征** | 可能作为文本节点 | 作为节点属性，GNN就绪 |
| **LLM输出** | 只保存解析后三元组 | 保存原始输出+多版本解析 |
| **Schema** | 固定11种类型 | 动态发现+版本管理 |
| **图谱** | 静态一次性构建 | 支持自拓展（实体链接+关系推理） |
| **导入** | 覆盖或追加 | 智能合并+增量更新 |
| **可修改性** | 需重新调用API | 本地重处理（免费） |

### 文件结构V2.0

```
automated_kg_pipeline/
├── llm_layer/
│   ├── api_caller.py                    # API调用（昂贵）
│   └── prompt_templates/                # 提示词模板化
│       ├── v1.0_base.txt
│       └── v2.0_with_numerical.txt
│
├── parsing_layer/
│   ├── json_parser.py                   # JSON解析
│   ├── numerical_extractor.py           # 数值特征抽取
│   └── schema_validator.py
│
├── post_processing_layer/
│   ├── synonym_mapper.py
│   ├── entity_deduplicator.py
│   └── feature_normalizer.py            # 特征归一化
│
├── augmentation_layer/                   # ← 新增
│   ├── entity_linker.py                 # 实体链接
│   ├── relation_inferencer.py           # 关系推理
│   ├── numerical_analyzer.py            # 数值聚类分析
│   └── schema_evolver.py                # Schema演化
│
├── neo4j_layer/
│   ├── import_manager.py
│   └── merge_strategies.py              # 合并策略
│
└── cache/
    ├── llm_raw_outputs/                 # ← 新增：原始LLM输出
    │   └── {article_id}_v1.json
    ├── parsed_triples/
    │   ├── {article_id}_v1.json         # 版本化
    │   └── {article_id}_v2.json
    └── augmented_graph/                 # ← 新增
        └── entity_links.json
```

---

## 启发式补充

### 补充1: 时序图谱（Temporal KG）

心脏移植是有**时序**的！

```cypher
// 手术流程时序
(:SurgeryStep {name: "体外循环建立", order: 1, duration_minutes: 15})
-[:TEMPORAL_NEXT {interval_minutes: 0}]->
(:SurgeryStep {name: "主动脉阻断", order: 2, duration_minutes: 5})
-[:TEMPORAL_NEXT]->
(:SurgeryStep {name: "病心切除", order: 3})

// 术后并发症时序
(:Case)-[:HAS_COMPLICATION {
  onset_time: "术后第3天",
  onset_hours: 72
}]->(:Complication {name: "急性排斥反应"})
```

**GNN应用**：时序图神经网络（TGNN）预测术后轨迹

### 补充2: 多模态图谱

```cypher
// 连接影像数据
(:Case)
-[:HAS_IMAGING]->
(:Imaging {
  type: "超声心动图",
  lvef: 15,
  image_path: "/data/echo/case001.dcm",
  image_embedding: [0.12, 0.34, ...]  // ViT特征
})

// 连接基因数据
(:Recipient)
-[:HAS_GENOTYPE]->
(:Genotype {
  hla_a: "A*02:01",
  hla_b: "B*44:02",
  mismatch_score: 2
})
```

### 补充3: 不确定性量化

```cypher
// 关系上的不确定性
(:Entity {name: "延长缺血时间"})
-[:导致 {
  confidence: 0.95,           // LLM置信度
  evidence_strength: "RCT",
  study_count: 15,            // 支持该关系的研究数
  consensus_score: 0.87       // 专家共识程度
}]->
(:Entity {name: "PGD"})
```

**GNN应用**：不确定性感知的GNN（Bayesian GNN）

### 补充4: 可解释性层

```cypher
// 决策证据链
(:Decision {
  case_id: "case_001",
  recommendation: "不推荐移植",
  risk_score: 0.85
})
-[:BASED_ON]->
(:Evidence {
  factor: "PVR过高",
  value: 6.2,
  threshold: 5.0,
  importance: 0.4  // 该因素对决策的贡献度
})
```

**连接到Agent系统**：
- Medical Expert提取Evidence
- GNN计算importance权重
- Reporter生成可解释报告

---

## 立即可执行的改进

### 优先级P0（立即修改）

1. **修改 `auto_kg_builder.py`**：
   - 保存原始LLM输出到 `llm_raw_outputs/`
   - 添加数值特征提取器

2. **创建 `numerical_extractor.py`**：
   - 正则表达式匹配数值
   - 转换为节点属性

3. **修改Neo4j导入逻辑**：
   - 支持merge模式
   - 区分概念节点和实例节点

### 优先级P1（V2.0版本）

4. **实现图谱拓展层**
5. **Schema演化分析器**
6. **版本管理系统**

---

## 总结：您的6个问题 → 系统性改进

| 问题 | 改进方案 | 优先级 |
|------|---------|-------|
| 1. 跑完得到什么 | 明确两层图结构：概念图+实例图 | P0 |
| 2. API修改成本 | 保存原始LLM输出，分层架构 | P0 |
| 3. 数据库覆盖 | 三种模式：overwrite/append/merge | P0 |
| 4. 数值作为属性 | 数值特征提取器 → 节点属性 | P0 |
| 5. 图谱自拓展 | 实体链接+关系推理+数值聚类 | P1 |
| 6. Schema灵活调整 | Schema演化+版本管理+增量重抽取 | P1 |

**建议执行路径**：
1. 先按当前V1.0跑一遍（验证基础流程）
2. 立即实施P0改进（数值特征、原始输出保存）
3. 在V2.0中实施图谱拓展和Schema演化

这样既保证进度，又为后续GNN和Agent架构留足灵活性。
