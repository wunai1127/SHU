"""
Agent 1 è®­ç»ƒè„šæœ¬
ç”¨äºå¾®è°ƒClinicalBERTå’Œè®­ç»ƒLSTMç¼–ç å™¨

è®­ç»ƒæ•°æ®æ ¼å¼:
- å¿ƒè„æè¿°æ–‡æœ¬ + æ ‡æ³¨çš„ç‰¹å¾ (hypertrophy, contractilityç­‰)
- è¡€æ°”æ—¶åºæ•°æ® + å¯¹åº”çš„æ ‡ç­¾
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import json
from pathlib import Path
from typing import Dict, List
from tqdm import tqdm
import numpy as np
from agent1_core import (
    ClinicalTextEncoder,
    BloodGasLSTMEncoder,
    InputUnderstandingAgent
)


class CardiacTextDataset(Dataset):
    """å¿ƒè„æè¿°æ–‡æœ¬æ•°æ®é›†"""
    def __init__(self, data_path: str):
        with open(data_path, 'r', encoding='utf-8') as f:
            self.data = json.load(f)

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]
        return {
            'text': item['text'],
            'hypertrophy': item['labels']['hypertrophy'],
            'contractility': item['labels']['contractility'],
            'valve_status': item['labels']['valve_status'],  # 0: good, 1: moderate, 2: poor
            'scarring': item['labels']['scarring'],
            'coronary_patency': item['labels']['coronary_patency']
        }


class BloodGasDataset(Dataset):
    """è¡€æ°”æ—¶åºæ•°æ®é›†"""
    def __init__(self, data_path: str):
        with open(data_path, 'r', encoding='utf-8') as f:
            self.data = json.load(f)

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]

        # æå–æ—¶åºåºåˆ—
        sequence = []
        for tp in item['sequence']:
            sequence.append([
                tp['lactate'],
                tp['pH'],
                tp['pO2'],
                tp['pCO2'],
                tp['K+'],
                tp['glucose']
            ])

        return {
            'sequence': torch.tensor(sequence, dtype=torch.float32),
            'outcome_score': item['outcome_score']  # 0-1
        }


class TextEncoderTrainer:
    """ClinicalBERTå¾®è°ƒè®­ç»ƒå™¨"""
    def __init__(self,
                 model: ClinicalTextEncoder,
                 device: str = 'cuda'):
        self.model = model.to(device)
        self.device = device

        # ä¼˜åŒ–å™¨ï¼ˆåªå¾®è°ƒfine_tune_layerå’Œfeature_extractorsï¼‰
        params_to_update = []
        for name, param in model.named_parameters():
            if 'bert' not in name:  # ä¸æ›´æ–°BERTå‚æ•°
                params_to_update.append(param)

        self.optimizer = optim.AdamW(params_to_update, lr=1e-4, weight_decay=0.01)

        # æŸå¤±å‡½æ•°
        self.mse_loss = nn.MSELoss()
        self.ce_loss = nn.CrossEntropyLoss()

    def train_epoch(self, dataloader: DataLoader) -> float:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0

        for batch in tqdm(dataloader, desc="Training"):
            self.optimizer.zero_grad()

            # å‰å‘ä¼ æ’­
            texts = batch['text']
            losses = []

            for i, text in enumerate(texts):
                embedding, features = self.model(text)

                # è®¡ç®—å„ç‰¹å¾çš„æŸå¤±
                # Hypertrophy
                hypertrophy_pred = torch.sigmoid(
                    self.model.feature_extractors['hypertrophy'](embedding.unsqueeze(0))
                )
                hypertrophy_loss = self.mse_loss(
                    hypertrophy_pred,
                    batch['hypertrophy'][i].unsqueeze(0).unsqueeze(1).to(self.device)
                )

                # Contractility
                contractility_pred = torch.sigmoid(
                    self.model.feature_extractors['contractility'](embedding.unsqueeze(0))
                )
                contractility_loss = self.mse_loss(
                    contractility_pred,
                    batch['contractility'][i].unsqueeze(0).unsqueeze(1).to(self.device)
                )

                # Valve status (åˆ†ç±»)
                valve_logits = self.model.feature_extractors['valve_status'](
                    embedding.unsqueeze(0)
                )
                valve_loss = self.ce_loss(
                    valve_logits,
                    batch['valve_status'][i].unsqueeze(0).to(self.device)
                )

                # Scarring
                scarring_pred = torch.sigmoid(
                    self.model.feature_extractors['scarring'](embedding.unsqueeze(0))
                )
                scarring_loss = self.mse_loss(
                    scarring_pred,
                    batch['scarring'][i].unsqueeze(0).unsqueeze(1).to(self.device)
                )

                # Coronary patency
                coronary_pred = torch.sigmoid(
                    self.model.feature_extractors['coronary_patency'](embedding.unsqueeze(0))
                )
                coronary_loss = self.mse_loss(
                    coronary_pred,
                    batch['coronary_patency'][i].unsqueeze(0).unsqueeze(1).to(self.device)
                )

                # æ€»æŸå¤±
                loss = (hypertrophy_loss + contractility_loss + valve_loss +
                       scarring_loss + coronary_loss) / 5

                losses.append(loss)

            # æ‰¹æ¬¡å¹³å‡æŸå¤±
            batch_loss = torch.stack(losses).mean()
            batch_loss.backward()
            self.optimizer.step()

            total_loss += batch_loss.item()

        return total_loss / len(dataloader)

    def save(self, path: str):
        """ä¿å­˜æ¨¡å‹"""
        torch.save(self.model.state_dict(), path)
        print(f"âœ… Model saved to {path}")


class LSTMEncoderTrainer:
    """LSTMç¼–ç å™¨è®­ç»ƒå™¨"""
    def __init__(self,
                 model: BloodGasLSTMEncoder,
                 device: str = 'cuda'):
        self.model = model.to(device)
        self.device = device

        self.optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)
        self.criterion = nn.MSELoss()

        # é¢„æµ‹å¤´ï¼ˆç”¨äºè®­ç»ƒï¼‰
        self.predictor = nn.Linear(256, 1).to(device)

    def train_epoch(self, dataloader: DataLoader) -> float:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0

        for batch in tqdm(dataloader, desc="Training LSTM"):
            self.optimizer.zero_grad()

            sequences = batch['sequence'].to(self.device)  # [batch, time, 6]
            outcomes = batch['outcome_score'].to(self.device)  # [batch]

            # å‰å‘ä¼ æ’­
            embeddings, attn_weights = self.model(sequences)

            # é¢„æµ‹ç»“å±€
            predictions = self.predictor(embeddings).squeeze()

            # æŸå¤±
            loss = self.criterion(predictions, outcomes)
            loss.backward()
            self.optimizer.step()

            total_loss += loss.item()

        return total_loss / len(dataloader)

    def save(self, path: str):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'encoder': self.model.state_dict(),
            'predictor': self.predictor.state_dict()
        }, path)
        print(f"âœ… LSTM model saved to {path}")


def train_text_encoder(data_path: str,
                       output_dir: str,
                       epochs: int = 5,
                       batch_size: int = 16):
    """
    è®­ç»ƒæ–‡æœ¬ç¼–ç å™¨

    Args:
        data_path: è®­ç»ƒæ•°æ®è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        epochs: è®­ç»ƒè½®æ•°
        batch_size: æ‰¹æ¬¡å¤§å°
    """
    print("=" * 60)
    print("è®­ç»ƒ ClinicalBERT æ–‡æœ¬ç¼–ç å™¨")
    print("=" * 60)

    # åˆ›å»ºè¾“å‡ºç›®å½•
    Path(output_dir).mkdir(parents=True, exist_ok=True)

    # åŠ è½½æ•°æ®
    dataset = CardiacTextDataset(data_path)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # åˆ›å»ºæ¨¡å‹å’Œè®­ç»ƒå™¨
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model = ClinicalTextEncoder()
    trainer = TextEncoderTrainer(model, device)

    # è®­ç»ƒ
    best_loss = float('inf')
    for epoch in range(epochs):
        print(f"\nğŸ“Š Epoch {epoch + 1}/{epochs}")
        loss = trainer.train_epoch(dataloader)
        print(f"   Loss: {loss:.4f}")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if loss < best_loss:
            best_loss = loss
            trainer.save(f"{output_dir}/text_encoder_best.pth")

    print(f"\nâœ… è®­ç»ƒå®Œæˆï¼æœ€ä½³Loss: {best_loss:.4f}")


def train_lstm_encoder(data_path: str,
                       output_dir: str,
                       epochs: int = 20,
                       batch_size: int = 32):
    """
    è®­ç»ƒLSTMç¼–ç å™¨

    Args:
        data_path: è®­ç»ƒæ•°æ®è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        epochs: è®­ç»ƒè½®æ•°
        batch_size: æ‰¹æ¬¡å¤§å°
    """
    print("=" * 60)
    print("è®­ç»ƒ LSTM æ—¶åºç¼–ç å™¨")
    print("=" * 60)

    # åˆ›å»ºè¾“å‡ºç›®å½•
    Path(output_dir).mkdir(parents=True, exist_ok=True)

    # åŠ è½½æ•°æ®
    dataset = BloodGasDataset(data_path)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # åˆ›å»ºæ¨¡å‹å’Œè®­ç»ƒå™¨
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model = BloodGasLSTMEncoder()
    trainer = LSTMEncoderTrainer(model, device)

    # è®­ç»ƒ
    best_loss = float('inf')
    for epoch in range(epochs):
        print(f"\nğŸ“Š Epoch {epoch + 1}/{epochs}")
        loss = trainer.train_epoch(dataloader)
        print(f"   Loss: {loss:.4f}")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if loss < best_loss:
            best_loss = loss
            trainer.save(f"{output_dir}/lstm_encoder_best.pth")

    print(f"\nâœ… è®­ç»ƒå®Œæˆï¼æœ€ä½³Loss: {best_loss:.4f}")


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Train Agent 1 components")
    parser.add_argument('--component', type=str, required=True,
                       choices=['text', 'lstm', 'both'],
                       help='è®­ç»ƒç»„ä»¶: text (æ–‡æœ¬ç¼–ç å™¨) / lstm (æ—¶åºç¼–ç å™¨) / both (ä¸¤è€…)')
    parser.add_argument('--text_data', type=str,
                       default='data/cardiac_text_train.json',
                       help='æ–‡æœ¬è®­ç»ƒæ•°æ®è·¯å¾„')
    parser.add_argument('--lstm_data', type=str,
                       default='data/blood_gas_train.json',
                       help='æ—¶åºè®­ç»ƒæ•°æ®è·¯å¾„')
    parser.add_argument('--output_dir', type=str,
                       default='checkpoints',
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--epochs', type=int, default=10,
                       help='è®­ç»ƒè½®æ•°')

    args = parser.parse_args()

    if args.component in ['text', 'both']:
        train_text_encoder(
            data_path=args.text_data,
            output_dir=args.output_dir,
            epochs=args.epochs
        )

    if args.component in ['lstm', 'both']:
        train_lstm_encoder(
            data_path=args.lstm_data,
            output_dir=args.output_dir,
            epochs=args.epochs if args.component == 'lstm' else 20
        )
