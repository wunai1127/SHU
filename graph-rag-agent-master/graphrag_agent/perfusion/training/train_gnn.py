#!/usr/bin/env python3
"""
GNN Training Script - Heart Perfusion Quality Prediction

GNN è®­ç»ƒè„šæœ¬ - å¿ƒè„çŒæ³¨è´¨é‡é¢„æµ‹æ¨¡å‹

=============================================================================
ä½¿ç”¨æ–¹æ³• (How to Run):
=============================================================================

1. å‡†å¤‡æ•°æ®åï¼Œç›´æ¥è¿è¡Œï¼š
   python train_gnn.py

2. æŒ‡å®šé…ç½®ï¼š
   python train_gnn.py --epochs 200 --batch_size 32 --lr 0.001

3. ä»checkpointæ¢å¤è®­ç»ƒï¼š
   python train_gnn.py --resume checkpoints/best_model.pt

4. ä½¿ç”¨TensorBoardæŸ¥çœ‹è®­ç»ƒæ›²çº¿ï¼š
   tensorboard --logdir=logs/

=============================================================================
æ•°æ®æ ¼å¼è¦æ±‚ (Data Format Requirements):
=============================================================================

CSV æ–‡ä»¶æ ¼å¼ï¼š
- æ¯è¡Œæ˜¯ä¸€ä¸ªæ—¶é—´ç‚¹çš„æµ‹é‡å€¼
- å¿…é¡»åŒ…å« case_id åˆ—æ¥åŒºåˆ†ä¸åŒç—…ä¾‹
- å¿…é¡»åŒ…å« timestamp åˆ—æ¥æ’åº
- å¿…é¡»åŒ…å«12ä¸ªç‰¹å¾åˆ—å’Œæ ‡ç­¾åˆ—

ç¤ºä¾‹ CSV:
case_id,timestamp,pH,PO2,PCO2,lactate,K_plus,Na_plus,IL_6,IL_8,TNF_alpha,pressure,flow_rate,temperature,quality_score,risk_level,usable
CASE001,0,7.40,400,40,1.5,4.0,140,10,8,5,60,1.5,34,85,low,1
CASE001,1,7.38,380,42,1.8,4.1,139,12,10,6,58,1.5,34,85,low,1
...

=============================================================================
"""

import os
import sys
import argparse
import json
import random
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
from torch.utils.tensorboard import SummaryWriter
from sklearn.metrics import accuracy_score, f1_score, mean_squared_error, r2_score
from tqdm import tqdm

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent.parent.parent))

from graphrag_agent.perfusion.temporal_gnn import (
    TemporalPerfusionGNN,
    TemporalGNNConfig,
    FeatureNormalizer,
    create_dummy_graph,
)
from graphrag_agent.perfusion.training.config import training_config, data_config


# =============================================================================
# ç¬¬ä¸€æ­¥ï¼šæ•°æ®é›†ç±»
# Step 1: Dataset Class
# =============================================================================

class PerfusionTrainingDataset(Dataset):
    """
    çŒæ³¨æ•°æ®é›†ç±»

    è¿™ä¸ªç±»è´Ÿè´£ï¼š
    1. ä»CSVåŠ è½½æ•°æ®
    2. æŒ‰ç—…ä¾‹åˆ†ç»„ï¼Œæ„å»ºæ—¶é—´åºåˆ—
    3. å½’ä¸€åŒ–ç‰¹å¾
    4. è¿”å›è®­ç»ƒæ‰€éœ€çš„å¼ é‡
    """

    def __init__(
        self,
        csv_path: str,
        seq_length: int = 20,
        feature_columns: List[str] = None,
        normalize: bool = True,
    ):
        """
        åˆå§‹åŒ–æ•°æ®é›†

        Args:
            csv_path: CSVæ–‡ä»¶è·¯å¾„
            seq_length: åºåˆ—é•¿åº¦ï¼ˆæ¯ä¸ªæ ·æœ¬åŒ…å«å¤šå°‘æ—¶é—´ç‚¹ï¼‰
            feature_columns: ç‰¹å¾åˆ—ååˆ—è¡¨
            normalize: æ˜¯å¦å½’ä¸€åŒ–ç‰¹å¾
        """
        print(f"Loading data from {csv_path}...")
        self.df = pd.read_csv(csv_path)
        self.seq_length = seq_length
        self.feature_columns = feature_columns or data_config.feature_columns

        # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
        # Check required columns exist
        self._validate_columns()

        # æŒ‰ç—…ä¾‹åˆ†ç»„ï¼Œæ„å»ºåºåˆ—
        # Group by case and build sequences
        self.samples = self._build_sequences()

        # å½’ä¸€åŒ–
        # Normalize
        self.normalize = normalize
        if normalize:
            self.normalizer = FeatureNormalizer()
            self._fit_normalizer()

        print(f"Loaded {len(self.samples)} samples from {len(self.df['case_id'].unique())} cases")

    def _validate_columns(self):
        """éªŒè¯CSVåŒ…å«æ‰€æœ‰å¿…è¦çš„åˆ—"""
        required = ['case_id', 'timestamp'] + self.feature_columns
        missing = [col for col in required if col not in self.df.columns]
        if missing:
            raise ValueError(f"Missing required columns: {missing}")

        # æ£€æŸ¥æ ‡ç­¾åˆ—ï¼ˆè‡³å°‘éœ€è¦ä¸€ä¸ªï¼‰
        label_cols = list(data_config.label_columns.values())
        if not any(col in self.df.columns for col in label_cols):
            raise ValueError(f"Need at least one label column from: {label_cols}")

    def _build_sequences(self) -> List[Dict]:
        """
        æŒ‰ç—…ä¾‹æ„å»ºæ—¶é—´åºåˆ—

        å°†æ¯ä¸ªç—…ä¾‹çš„æµ‹é‡å€¼æŒ‰æ—¶é—´æ’åºï¼Œç„¶ååˆ‡åˆ†æˆå›ºå®šé•¿åº¦çš„åºåˆ—
        """
        samples = []

        for case_id, group in self.df.groupby('case_id'):
            # æŒ‰æ—¶é—´æ’åº
            group = group.sort_values('timestamp')

            # æå–ç‰¹å¾çŸ©é˜µ
            features = group[self.feature_columns].values

            # å¦‚æœåºåˆ—å¤ªçŸ­ï¼Œè·³è¿‡æˆ–å¡«å……
            if len(features) < self.seq_length:
                # ç”¨æœ€åä¸€è¡Œå¡«å……
                padding = np.repeat(
                    features[-1:],
                    self.seq_length - len(features),
                    axis=0
                )
                features = np.vstack([features, padding])

            # æå–æ ‡ç­¾ï¼ˆä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´ç‚¹çš„æ ‡ç­¾ï¼‰
            labels = {}
            if 'quality_score' in group.columns:
                labels['quality_score'] = float(group['quality_score'].iloc[-1])
            if 'risk_level' in group.columns:
                risk_map = {'low': 0, 'medium': 1, 'high': 2, 'critical': 3}
                risk_str = group['risk_level'].iloc[-1]
                labels['risk_level'] = risk_map.get(risk_str, 1)
            if 'usable' in group.columns:
                labels['usable'] = int(group['usable'].iloc[-1])

            # æ»‘åŠ¨çª—å£é‡‡æ ·ï¼ˆå¦‚æœåºåˆ—å¾ˆé•¿ï¼Œå¯ä»¥è·å¾—å¤šä¸ªæ ·æœ¬ï¼‰
            for start_idx in range(0, len(features) - self.seq_length + 1, self.seq_length // 2):
                end_idx = start_idx + self.seq_length
                sample = {
                    'case_id': case_id,
                    'features': features[start_idx:end_idx].copy(),
                    'labels': labels.copy(),
                }
                samples.append(sample)

        return samples

    def _fit_normalizer(self):
        """æ‹Ÿåˆå½’ä¸€åŒ–å™¨ï¼ˆè®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®ï¼‰"""
        all_features = np.vstack([s['features'] for s in self.samples])
        self.normalizer.fit(all_features)

    def __len__(self) -> int:
        return len(self.samples)

    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        """
        è·å–ä¸€ä¸ªæ ·æœ¬

        Returns:
            Dict with:
            - 'features': [seq_length, num_features] ç‰¹å¾å¼ é‡
            - 'quality_score': scalar è´¨é‡åˆ†æ•°
            - 'risk_level': scalar é£é™©ç­‰çº§ (0-3)
            - 'usable': scalar æ˜¯å¦å¯ç”¨ (0/1)
        """
        sample = self.samples[idx]
        features = sample['features'].copy()

        # å½’ä¸€åŒ–
        if self.normalize:
            features = self.normalizer.transform(features)

        result = {
            'features': torch.tensor(features, dtype=torch.float32),
        }

        # æ·»åŠ æ ‡ç­¾
        labels = sample['labels']
        if 'quality_score' in labels:
            result['quality_score'] = torch.tensor(labels['quality_score'] / 100.0, dtype=torch.float32)
        if 'risk_level' in labels:
            result['risk_level'] = torch.tensor(labels['risk_level'], dtype=torch.long)
        if 'usable' in labels:
            result['usable'] = torch.tensor(labels['usable'], dtype=torch.float32)

        return result


# =============================================================================
# ç¬¬äºŒæ­¥ï¼šè®­ç»ƒå™¨ç±»
# Step 2: Trainer Class
# =============================================================================

class GNNTrainer:
    """
    GNNæ¨¡å‹è®­ç»ƒå™¨

    è´Ÿè´£ï¼š
    1. æ¨¡å‹åˆå§‹åŒ–
    2. è®­ç»ƒå¾ªç¯
    3. éªŒè¯å’Œæµ‹è¯•
    4. ä¿å­˜æ£€æŸ¥ç‚¹
    5. TensorBoardæ—¥å¿—
    """

    def __init__(
        self,
        model: nn.Module,
        train_loader: DataLoader,
        val_loader: DataLoader,
        test_loader: DataLoader = None,
        config: 'GNNTrainingConfig' = None,
        device: str = 'cuda',
    ):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨

        Args:
            model: GNNæ¨¡å‹
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨ï¼ˆå¯é€‰ï¼‰
            config: è®­ç»ƒé…ç½®
            device: è®­ç»ƒè®¾å¤‡ ('cuda' or 'cpu')
        """
        self.config = config or training_config
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        print(f"Using device: {self.device}")

        # æ¨¡å‹
        self.model = model.to(self.device)

        # æ•°æ®åŠ è½½å™¨
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.test_loader = test_loader

        # ä¼˜åŒ–å™¨ - Adam with weight decay
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=self.config.learning_rate,
            weight_decay=self.config.weight_decay,
        )

        # å­¦ä¹ ç‡è°ƒåº¦å™¨ - éªŒè¯lossä¸ä¸‹é™æ—¶é™ä½å­¦ä¹ ç‡
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer,
            mode='min',
            factor=0.5,       # é™ä½ä¸ºåŸæ¥çš„ä¸€åŠ
            patience=5,       # 5è½®ä¸ä¸‹é™å°±é™ä½
            verbose=True,
        )

        # æŸå¤±å‡½æ•°
        self.quality_loss = nn.MSELoss()           # è´¨é‡åˆ†æ•° - å›å½’
        self.risk_loss = nn.CrossEntropyLoss()     # é£é™©ç­‰çº§ - åˆ†ç±»
        self.usable_loss = nn.BCEWithLogitsLoss()  # æ˜¯å¦å¯ç”¨ - äºŒåˆ†ç±»

        # TensorBoard
        log_dir = Path(self.config.log_dir) / datetime.now().strftime('%Y%m%d_%H%M%S')
        self.writer = SummaryWriter(log_dir)
        print(f"TensorBoard logs: {log_dir}")

        # æ£€æŸ¥ç‚¹ç›®å½•
        self.checkpoint_dir = Path(self.config.checkpoint_dir)
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)

        # æ—©åœç›¸å…³
        self.best_val_loss = float('inf')
        self.patience_counter = 0

        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦ä½¿ç”¨å›¾ç¼–ç å™¨
        # Check if model uses graph encoder
        self.use_graph = getattr(model, 'use_graph_encoder', True)

        # è™šæ‹Ÿå›¾ï¼ˆä»…åœ¨ä½¿ç”¨å›¾ç¼–ç å™¨æ—¶éœ€è¦ï¼‰
        # Dummy graph (only needed when using graph encoder)
        if self.use_graph:
            self.dummy_graph = create_dummy_graph().to(self.device)
        else:
            self.dummy_graph = None

    def train_epoch(self, epoch: int) -> Dict[str, float]:
        """
        è®­ç»ƒä¸€ä¸ªepoch

        Args:
            epoch: å½“å‰epochç¼–å·

        Returns:
            Dict of metrics
        """
        self.model.train()
        total_loss = 0
        quality_losses = []
        risk_losses = []

        # è¿›åº¦æ¡
        pbar = tqdm(self.train_loader, desc=f'Epoch {epoch+1}')

        for batch_idx, batch in enumerate(pbar):
            # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
            features = batch['features'].to(self.device)

            # å‰å‘ä¼ æ’­
            outputs = self.model(features, self.dummy_graph)

            # è®¡ç®—æŸå¤±
            loss = 0

            # è´¨é‡åˆ†æ•°æŸå¤±
            if 'quality_score' in batch:
                quality_target = batch['quality_score'].to(self.device)
                quality_pred = outputs['quality_score'].squeeze()
                q_loss = self.quality_loss(quality_pred, quality_target)
                loss += q_loss
                quality_losses.append(q_loss.item())

            # é£é™©ç­‰çº§æŸå¤±
            if 'risk_level' in batch:
                risk_target = batch['risk_level'].to(self.device)
                risk_pred = outputs['risk_logits']
                r_loss = self.risk_loss(risk_pred, risk_target)
                loss += r_loss
                risk_losses.append(r_loss.item())

            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()

            # æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)

            self.optimizer.step()

            total_loss += loss.item()

            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'q_loss': f'{np.mean(quality_losses):.4f}' if quality_losses else 'N/A',
            })

        avg_loss = total_loss / len(self.train_loader)
        return {
            'loss': avg_loss,
            'quality_loss': np.mean(quality_losses) if quality_losses else 0,
            'risk_loss': np.mean(risk_losses) if risk_losses else 0,
        }

    @torch.no_grad()
    def validate(self) -> Dict[str, float]:
        """
        åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°

        Returns:
            Dict of metrics
        """
        self.model.eval()
        total_loss = 0
        all_quality_preds = []
        all_quality_targets = []
        all_risk_preds = []
        all_risk_targets = []

        for batch in self.val_loader:
            features = batch['features'].to(self.device)
            outputs = self.model(features, self.dummy_graph)

            loss = 0

            if 'quality_score' in batch:
                quality_target = batch['quality_score'].to(self.device)
                quality_pred = outputs['quality_score'].squeeze()
                loss += self.quality_loss(quality_pred, quality_target)

                all_quality_preds.extend(quality_pred.cpu().numpy())
                all_quality_targets.extend(quality_target.cpu().numpy())

            if 'risk_level' in batch:
                risk_target = batch['risk_level'].to(self.device)
                risk_pred = outputs['risk_logits']
                loss += self.risk_loss(risk_pred, risk_target)

                pred_class = risk_pred.argmax(dim=1)
                all_risk_preds.extend(pred_class.cpu().numpy())
                all_risk_targets.extend(risk_target.cpu().numpy())

            total_loss += loss.item()

        avg_loss = total_loss / len(self.val_loader)

        metrics = {'loss': avg_loss}

        # è®¡ç®—å›å½’æŒ‡æ ‡
        if all_quality_preds:
            mse = mean_squared_error(all_quality_targets, all_quality_preds)
            r2 = r2_score(all_quality_targets, all_quality_preds)
            metrics['quality_mse'] = mse
            metrics['quality_r2'] = r2

        # è®¡ç®—åˆ†ç±»æŒ‡æ ‡
        if all_risk_preds:
            acc = accuracy_score(all_risk_targets, all_risk_preds)
            f1 = f1_score(all_risk_targets, all_risk_preds, average='weighted')
            metrics['risk_accuracy'] = acc
            metrics['risk_f1'] = f1

        return metrics

    def save_checkpoint(self, epoch: int, is_best: bool = False):
        """
        ä¿å­˜æ£€æŸ¥ç‚¹

        Args:
            epoch: å½“å‰epoch
            is_best: æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹
        """
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'best_val_loss': self.best_val_loss,
            'config': self.config.__dict__,
        }

        # ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹
        latest_path = self.checkpoint_dir / 'latest_checkpoint.pt'
        torch.save(checkpoint, latest_path)

        # å¦‚æœæ˜¯æœ€ä½³æ¨¡å‹ï¼Œé¢å¤–ä¿å­˜ä¸€ä»½
        if is_best:
            best_path = self.checkpoint_dir / 'best_model.pt'
            torch.save(checkpoint, best_path)
            print(f"  ğŸ’¾ Saved best model to {best_path}")

    def load_checkpoint(self, checkpoint_path: str):
        """
        åŠ è½½æ£€æŸ¥ç‚¹ï¼ˆç”¨äºæ¢å¤è®­ç»ƒï¼‰

        Args:
            checkpoint_path: æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„
        """
        print(f"Loading checkpoint from {checkpoint_path}...")
        checkpoint = torch.load(checkpoint_path, map_location=self.device)

        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        self.best_val_loss = checkpoint['best_val_loss']

        return checkpoint['epoch']

    def train(self, num_epochs: int = None, resume_from: str = None) -> Dict:
        """
        å®Œæ•´è®­ç»ƒæµç¨‹

        Args:
            num_epochs: è®­ç»ƒè½®æ•°
            resume_from: ä»å“ªä¸ªæ£€æŸ¥ç‚¹æ¢å¤

        Returns:
            è®­ç»ƒå†å²
        """
        num_epochs = num_epochs or self.config.num_epochs
        start_epoch = 0

        # æ¢å¤è®­ç»ƒ
        if resume_from:
            start_epoch = self.load_checkpoint(resume_from) + 1
            print(f"Resuming from epoch {start_epoch}")

        history = {
            'train_loss': [],
            'val_loss': [],
            'val_metrics': [],
        }

        print("=" * 60)
        print("Starting Training")
        print("=" * 60)

        for epoch in range(start_epoch, num_epochs):
            # è®­ç»ƒ
            train_metrics = self.train_epoch(epoch)
            history['train_loss'].append(train_metrics['loss'])

            # éªŒè¯
            val_metrics = self.validate()
            history['val_loss'].append(val_metrics['loss'])
            history['val_metrics'].append(val_metrics)

            # å­¦ä¹ ç‡è°ƒåº¦
            self.scheduler.step(val_metrics['loss'])

            # è®°å½•åˆ°TensorBoard
            self.writer.add_scalar('Loss/train', train_metrics['loss'], epoch)
            self.writer.add_scalar('Loss/val', val_metrics['loss'], epoch)
            if 'quality_r2' in val_metrics:
                self.writer.add_scalar('Metrics/quality_r2', val_metrics['quality_r2'], epoch)
            if 'risk_accuracy' in val_metrics:
                self.writer.add_scalar('Metrics/risk_accuracy', val_metrics['risk_accuracy'], epoch)
            self.writer.add_scalar('LR', self.optimizer.param_groups[0]['lr'], epoch)

            # æ‰“å°è¿›åº¦
            print(f"\nEpoch {epoch+1}/{num_epochs}")
            print(f"  Train Loss: {train_metrics['loss']:.4f}")
            print(f"  Val Loss:   {val_metrics['loss']:.4f}")
            if 'quality_r2' in val_metrics:
                print(f"  Quality RÂ²: {val_metrics['quality_r2']:.4f}")
            if 'risk_accuracy' in val_metrics:
                print(f"  Risk Acc:   {val_metrics['risk_accuracy']:.4f}")

            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹
            is_best = val_metrics['loss'] < self.best_val_loss - self.config.min_delta
            if is_best:
                self.best_val_loss = val_metrics['loss']
                self.patience_counter = 0
            else:
                self.patience_counter += 1

            # ä¿å­˜æ£€æŸ¥ç‚¹
            self.save_checkpoint(epoch, is_best)

            # æ—©åœæ£€æŸ¥
            if self.patience_counter >= self.config.patience:
                print(f"\nâš ï¸ Early stopping at epoch {epoch+1}")
                print(f"   Validation loss hasn't improved for {self.config.patience} epochs")
                break

        # å…³é—­TensorBoard
        self.writer.close()

        print("\n" + "=" * 60)
        print("Training Complete!")
        print(f"Best validation loss: {self.best_val_loss:.4f}")
        print(f"Model saved to: {self.checkpoint_dir / 'best_model.pt'}")
        print("=" * 60)

        return history

    @torch.no_grad()
    def test(self) -> Dict[str, float]:
        """
        åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æœ€ç»ˆæ¨¡å‹

        Returns:
            æµ‹è¯•æŒ‡æ ‡
        """
        if self.test_loader is None:
            print("No test loader provided")
            return {}

        # åŠ è½½æœ€ä½³æ¨¡å‹
        best_path = self.checkpoint_dir / 'best_model.pt'
        if best_path.exists():
            self.load_checkpoint(str(best_path))

        self.model.eval()

        all_quality_preds = []
        all_quality_targets = []
        all_risk_preds = []
        all_risk_targets = []

        for batch in tqdm(self.test_loader, desc='Testing'):
            features = batch['features'].to(self.device)
            outputs = self.model(features, self.dummy_graph)

            if 'quality_score' in batch:
                quality_pred = outputs['quality_score'].squeeze()
                all_quality_preds.extend(quality_pred.cpu().numpy())
                all_quality_targets.extend(batch['quality_score'].numpy())

            if 'risk_level' in batch:
                risk_pred = outputs['risk_logits'].argmax(dim=1)
                all_risk_preds.extend(risk_pred.cpu().numpy())
                all_risk_targets.extend(batch['risk_level'].numpy())

        metrics = {}

        if all_quality_preds:
            # è½¬æ¢å› 0-100 åˆ†æ•°
            preds_100 = np.array(all_quality_preds) * 100
            targets_100 = np.array(all_quality_targets) * 100

            metrics['quality_mse'] = mean_squared_error(targets_100, preds_100)
            metrics['quality_rmse'] = np.sqrt(metrics['quality_mse'])
            metrics['quality_r2'] = r2_score(targets_100, preds_100)
            metrics['quality_mae'] = np.mean(np.abs(targets_100 - preds_100))

        if all_risk_preds:
            metrics['risk_accuracy'] = accuracy_score(all_risk_targets, all_risk_preds)
            metrics['risk_f1'] = f1_score(all_risk_targets, all_risk_preds, average='weighted')

        print("\n" + "=" * 60)
        print("Test Results")
        print("=" * 60)
        for k, v in metrics.items():
            print(f"  {k}: {v:.4f}")

        return metrics


# =============================================================================
# ç¬¬ä¸‰æ­¥ï¼šåˆ›å»ºæ¨¡æ‹Ÿæ•°æ®ï¼ˆç”¨äºæµ‹è¯•ï¼‰
# Step 3: Create Synthetic Data (for testing)
# =============================================================================

def create_synthetic_data(
    num_cases: int = 100,
    seq_length: int = 20,
    output_path: str = './data/synthetic_perfusion.csv'
) -> str:
    """
    åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®ç”¨äºæµ‹è¯•è®­ç»ƒæµç¨‹

    ç”Ÿæˆç¬¦åˆçœŸå®åˆ†å¸ƒçš„æ¨¡æ‹ŸçŒæ³¨æ•°æ®

    Args:
        num_cases: ç—…ä¾‹æ•°é‡
        seq_length: æ¯ä¸ªç—…ä¾‹çš„æ—¶é—´ç‚¹æ•°é‡
        output_path: è¾“å‡ºCSVè·¯å¾„

    Returns:
        è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    print(f"Creating synthetic data with {num_cases} cases...")

    np.random.seed(42)
    rows = []

    for case_idx in range(num_cases):
        case_id = f"CASE{case_idx:04d}"

        # éšæœºå†³å®šè¿™ä¸ªå¿ƒè„çš„åŸºçº¿çŠ¶æ€
        # å¥½å¿ƒè„ vs å·®å¿ƒè„
        is_good = np.random.random() > 0.3  # 70% æ˜¯å¥½å¿ƒè„

        # åŸºçº¿å€¼
        if is_good:
            base_ph = 7.38 + np.random.normal(0, 0.02)
            base_lactate = 1.5 + np.random.normal(0, 0.3)
            quality_score = 75 + np.random.normal(0, 10)
            risk_level = np.random.choice(['low', 'medium'], p=[0.7, 0.3])
        else:
            base_ph = 7.28 + np.random.normal(0, 0.04)
            base_lactate = 3.5 + np.random.normal(0, 0.8)
            quality_score = 45 + np.random.normal(0, 15)
            risk_level = np.random.choice(['medium', 'high', 'critical'], p=[0.3, 0.5, 0.2])

        quality_score = np.clip(quality_score, 0, 100)
        usable = 1 if quality_score >= 60 else 0

        # ç”Ÿæˆæ—¶é—´åºåˆ—
        for t in range(seq_length):
            # æ·»åŠ æ—¶é—´è¶‹åŠ¿å’Œå™ªå£°
            trend = t / seq_length * 0.1  # è½»å¾®ä¸‹é™è¶‹åŠ¿

            row = {
                'case_id': case_id,
                'timestamp': t,
                'pH': base_ph - trend * 0.1 + np.random.normal(0, 0.01),
                'PO2': 400 - trend * 50 + np.random.normal(0, 20),
                'PCO2': 40 + trend * 5 + np.random.normal(0, 3),
                'lactate': base_lactate + trend * 1.5 + np.random.normal(0, 0.2),
                'K_plus': 4.0 + trend * 0.3 + np.random.normal(0, 0.1),
                'Na_plus': 140 - trend * 2 + np.random.normal(0, 1),
                'IL_6': 15 + trend * 20 + np.random.exponential(5),
                'IL_8': 10 + trend * 15 + np.random.exponential(3),
                'TNF_alpha': 8 + trend * 10 + np.random.exponential(2),
                'pressure': 60 - trend * 10 + np.random.normal(0, 3),
                'flow_rate': 1.5 - trend * 0.2 + np.random.normal(0, 0.1),
                'temperature': 34 + trend * 2 + np.random.normal(0, 0.5),
                'quality_score': quality_score,
                'risk_level': risk_level,
                'usable': usable,
            }
            rows.append(row)

    df = pd.DataFrame(rows)

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    Path(output_path).parent.mkdir(parents=True, exist_ok=True)

    df.to_csv(output_path, index=False)
    print(f"Saved synthetic data to {output_path}")
    print(f"  - Total rows: {len(df)}")
    print(f"  - Unique cases: {df['case_id'].nunique()}")

    return output_path


# =============================================================================
# ç¬¬å››æ­¥ï¼šä¸»å‡½æ•°
# Step 4: Main Function
# =============================================================================

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='Train GNN for Perfusion Prediction')

    # æ•°æ®å‚æ•°
    parser.add_argument('--data', type=str, default='./data/synthetic_perfusion.csv',
                       help='Path to training data CSV')
    parser.add_argument('--create_synthetic', action='store_true',
                       help='Create synthetic data for testing')
    parser.add_argument('--num_cases', type=int, default=200,
                       help='Number of synthetic cases to create')

    # è®­ç»ƒå‚æ•°
    parser.add_argument('--epochs', type=int, default=training_config.num_epochs,
                       help='Number of training epochs')
    parser.add_argument('--batch_size', type=int, default=training_config.batch_size,
                       help='Batch size')
    parser.add_argument('--lr', type=float, default=training_config.learning_rate,
                       help='Learning rate')
    parser.add_argument('--seq_length', type=int, default=training_config.seq_length,
                       help='Sequence length')

    # å…¶ä»–å‚æ•°
    parser.add_argument('--resume', type=str, default=None,
                       help='Path to checkpoint to resume from')
    parser.add_argument('--device', type=str, default='cuda',
                       help='Device to use (cuda or cpu)')
    parser.add_argument('--seed', type=int, default=42,
                       help='Random seed')

    args = parser.parse_args()

    # è®¾ç½®éšæœºç§å­
    random.seed(args.seed)
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(args.seed)

    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if args.create_synthetic or not Path(args.data).exists():
        print("\nâš ï¸ Training data not found, creating synthetic data...")
        args.data = create_synthetic_data(num_cases=args.num_cases)

    # åŠ è½½æ•°æ®é›†
    print("\n" + "=" * 60)
    print("Loading Dataset")
    print("=" * 60)

    dataset = PerfusionTrainingDataset(
        csv_path=args.data,
        seq_length=args.seq_length,
    )

    # åˆ’åˆ†æ•°æ®é›†
    total_size = len(dataset)
    train_size = int(total_size * training_config.train_split)
    val_size = int(total_size * training_config.val_split)
    test_size = total_size - train_size - val_size

    train_dataset, val_dataset, test_dataset = random_split(
        dataset,
        [train_size, val_size, test_size],
        generator=torch.Generator().manual_seed(args.seed)
    )

    print(f"Dataset splits: train={train_size}, val={val_size}, test={test_size}")

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=4,
        pin_memory=True,
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=4,
    )

    test_loader = DataLoader(
        test_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=4,
    )

    # åˆ›å»ºæ¨¡å‹
    print("\n" + "=" * 60)
    print("Creating Model")
    print("=" * 60)

    gnn_config = TemporalGNNConfig(
        hidden_dim=training_config.hidden_dim,
        num_layers=training_config.num_layers,
        dropout=training_config.dropout,
        num_features=training_config.num_features,
        # ç¦ç”¨å›¾ç¼–ç å™¨ï¼Œå› ä¸ºç›®å‰æ²¡æœ‰çœŸå®çš„çŸ¥è¯†å›¾è°±æ•°æ®
        # åªä½¿ç”¨ LSTM æ—¶åºç¼–ç å™¨è¿›è¡Œè®­ç»ƒ
        # Disable graph encoder since we don't have real knowledge graph data
        # Only use LSTM temporal encoder for training
        use_graph_encoder=False,
    )

    model = TemporalPerfusionGNN(gnn_config)
    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"Model parameters: {num_params:,}")

    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = GNNTrainer(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        test_loader=test_loader,
        config=training_config,
        device=args.device,
    )

    # è®­ç»ƒ
    history = trainer.train(
        num_epochs=args.epochs,
        resume_from=args.resume,
    )

    # æµ‹è¯•
    test_metrics = trainer.test()

    # ä¿å­˜è®­ç»ƒå†å²
    history_path = Path(training_config.checkpoint_dir) / 'training_history.json'
    with open(history_path, 'w') as f:
        json.dump({
            'train_loss': history['train_loss'],
            'val_loss': history['val_loss'],
            'test_metrics': test_metrics,
        }, f, indent=2)

    print(f"\nTraining history saved to {history_path}")


if __name__ == '__main__':
    main()
